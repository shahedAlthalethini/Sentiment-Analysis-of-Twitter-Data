{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Programming Languages- DSAI 1303 \n",
    "## Course Project: Sentiment Analysis of Twitter Data\n",
    "\n",
    "Twitter has emerged as a fundamentally new instrument to obtain social measurements. For example, researchers have shown that the \"mood\" of communication on twitter can be used to predict the stock market. \n",
    "\n",
    "In this programming project you will:\n",
    "\n",
    "* Load and prepare a collected set of twitter data for analysis\n",
    "* You will estimate the sentiment associated with individual tweets\n",
    "* You will estimate the sentiment of a particular term\n",
    "\n",
    "Please keep in mind the following points:\n",
    "* This assignment is open-ended in several ways. You will need to make some decisions about how to best solve each of the problems mentioned above.\n",
    "* **It is absolutely fine to discuss your solutions with your classmates but you are not allowed to share code.**\n",
    "* **Each student must submit their own solution via Google Classroom.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting of Twitter Data\n",
    "\n",
    "Strings in the twitter data prefixed with the letter \"u\" are unicode strings. For example: `u\"This is a string\"`.\n",
    "\n",
    "Unicode is a standard for representing a mach larger variety of characters beyond the roma alphabet (greek, russian, mathematical symbols, logograms from non-phonetic writing systems, etc.).\n",
    "\n",
    "In most circumstances, you will be able to use a unicode object just like a string.\n",
    "\n",
    "If you encounter an error involving printing unicode, you can use the [encode](https://docs.python.org/3/library/stdtypes.html#str.encode) method to properly print the international characters. You can find more information about UNICODE and Python 3 [here](https://docs.python.org/3/howto/unicode.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Loading and Cleaning Twitter Data [20 points]\n",
    "\n",
    "In this first part, you will neeed to load a sample of tweets in memory and prepare them for analysis. The tweets are stored in the file `tweets.json`. This file follows the *JSON* format. JSON stands for JavaScript Object Notation. It is a simple format for representing nested structres of data --- lists of lists of dictionaries of lists of ... you get the idea.\n",
    "\n",
    "Each line in of `tweets.json` represents a message. It is straightforward to convert a JSON string into a Python data structure; there is a library to do so called `json`. Below we will show you how to load the data and how to parse the first line in the `tweets.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Zu_Noma You fantasize about dating a dentist?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>At the dentist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>will you stop raining let me go to my dentist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are naughty, I shall bite you. And the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@mmpadellan This is the adult take.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dentists do have emergencies.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Dentist Offers Treatments Options Be sure ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@faezmaleksss visit yr dentist every 6 months ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dentist Lawrence Neville reveals foods to avoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@TotalRaritrash Will never see here anything l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i cant be the only dentist who couldnt care le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>First dentist to become a chief minister. http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>They do the same with Mexico. Americans are us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Went to dentist today and his face was so funn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>feeling very dentist stuffies w teeth horrorco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>omgg that’s so cool if u became a dentist!!! i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sadly... long line at the dentist..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dentists will love Jake's teeth me thinks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@teansprite Yup and also the dentist scene whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>twit content</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0      @Zu_Noma You fantasize about dating a dentist?\n",
       "1                                     At the dentist \n",
       "2       will you stop raining let me go to my dentist\n",
       "3   If you are naughty, I shall bite you. And the ...\n",
       "4                 @mmpadellan This is the adult take.\n",
       "5                       Dentists do have emergencies.\n",
       "6   The Dentist Offers Treatments Options Be sure ...\n",
       "7   @faezmaleksss visit yr dentist every 6 months ...\n",
       "8   Dentist Lawrence Neville reveals foods to avoi...\n",
       "9   @TotalRaritrash Will never see here anything l...\n",
       "10  i cant be the only dentist who couldnt care le...\n",
       "11  First dentist to become a chief minister. http...\n",
       "12  They do the same with Mexico. Americans are us...\n",
       "13  Went to dentist today and his face was so funn...\n",
       "14  feeling very dentist stuffies w teeth horrorco...\n",
       "15  omgg that’s so cool if u became a dentist!!! i...\n",
       "16                Sadly... long line at the dentist..\n",
       "17          dentists will love Jake's teeth me thinks\n",
       "18  @teansprite Yup and also the dentist scene whe...\n",
       "19                                       twit content"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "tweets = pd.read_csv(r\"C:\\Users\\best tech\\Desktop\\مشروع مادة البرمجة الفصل الثاني\\course project programming - الورقة1.csv\",\",\",names=[\"text\"])\n",
    "tweets.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  you fantasize about dating a dentist\n",
       "1                                       at the dentist \n",
       "2         will you stop raining let me go to my dentist\n",
       "3     if you are naughty, i shall bite you. and the ...\n",
       "4                                this is the adult take\n",
       "5                          dentists do have emergencies\n",
       "6     the dentist offers treatments options be sure ...\n",
       "7           visit yr dentist every 6 months if you can \n",
       "8     dentist lawrence neville reveals foods to avoi...\n",
       "9       will never see here anything less then the f...\n",
       "10    i cant be the only dentist who couldnt care le...\n",
       "11          first dentist to become a chief minister.  \n",
       "12    they do the same with mexico. americans are us...\n",
       "13    went to dentist today and his face was so funn...\n",
       "14    feeling very dentist stuffies w teeth horrorco...\n",
       "15    omgg that’s so cool if u became a dentist!!! i...\n",
       "16                    sadly... long line at the dentist\n",
       "17            dentists will love jake's teeth me thinks\n",
       "18      yup and also the dentist scene when anne was...\n",
       "19                                         twit content\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_datafile = open(\"clean.txt\",\"w\")\n",
    "import re\n",
    "def X(x):\n",
    "    e1 = re.sub(r\"@\\S*\",\" \",x)               \n",
    "    e2 = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\",\" \",e1)\n",
    "    word = e2.strip(\".,;!%&'#$?\")\n",
    "    k = word.lower()\n",
    "    return k\n",
    "text_tweets = tweets[\"text\"].apply(X)\n",
    "tweet_clean = pd.DataFrame(text_tweets)\n",
    "clean_datafile.write(str(tweet_clean[\"text\"]))\n",
    "clean_datafile.close()\n",
    "text_tweets.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry in `tweets.json`, i.e., each `tweet`, corresponds to a dictionary that contains lots of information about the tweet, the user, the activity related to the tweet (i.e., if it was retweeted or not), the timestamp of the tweet, entities mentioned in the tweet, hashtags used, etc.\n",
    "\n",
    "You can treat the `tweet` variable from above as a dicitonary and use the `.keys()` command to see the fields associated with the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select any of the aforemented values of Variable `tweet` by treating it as a dictionary. For example let's select the `text` body of the tweet, the time it was `created_at`, and the `hashtags` it contains.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this tweet contains no hashtags. The body of the tweet contains several information that is not necesary for our sentiment analysis task. For example, it contains a comma, a reference to a twitter user and a link to an external website. \n",
    "\n",
    "Since this information is not necessary we can remove it. In other words we need to clean our input in order to prepare it for analysis. Next, we show you some basic cleaning operations using **regular expressions**. You can find more information on regular expressions [here](https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you\n",
      "fantasize\n",
      "about\n",
      "dating\n",
      "a\n",
      "dentist\n",
      "at\n",
      "the\n",
      "dentist\n",
      "will\n",
      "you\n",
      "stop\n",
      "raining\n",
      "let\n",
      "me\n",
      "go\n",
      "to\n",
      "my\n",
      "dentist\n",
      "if\n",
      "you\n",
      "are\n",
      "naughty\n",
      "i\n",
      "shall\n",
      "bite\n",
      "you\n",
      "and\n",
      "the\n",
      "dentist\n",
      "tells\n",
      "me\n",
      "i\n",
      "have\n",
      "one\n",
      "of\n",
      "the\n",
      "strongest\n",
      "bites\n",
      "he\n",
      "knows\n",
      "so\n",
      "beware\n",
      "this\n",
      "is\n",
      "the\n",
      "adult\n",
      "take\n",
      "dentists\n",
      "do\n",
      "have\n",
      "emergencies\n",
      "the\n",
      "dentist\n",
      "offers\n",
      "treatments\n",
      "options\n",
      "be\n",
      "sure\n",
      "that\n",
      "the\n",
      "dentist\n",
      "provides\n",
      "treatment\n",
      "options\n",
      "that\n",
      "fit\n",
      "your\n",
      "specific\n",
      "needs\n",
      "and\n",
      "your\n",
      "budget\n",
      "coast\n",
      "dental\n",
      "gives\n",
      "every\n",
      "patient\n",
      "a\n",
      "written\n",
      "treatment\n",
      "plan\n",
      "to\n",
      "review\n",
      "before\n",
      "any\n",
      "procedure\n",
      "begins\n",
      "visit\n",
      "yr\n",
      "dentist\n",
      "every\n",
      "months\n",
      "if\n",
      "you\n",
      "can\n",
      "dentist\n",
      "lawrence\n",
      "neville\n",
      "reveals\n",
      "foods\n",
      "to\n",
      "avoid\n",
      "if\n",
      "you\n",
      "want\n",
      "a\n",
      "perfect\n",
      "smile\n",
      "will\n",
      "never\n",
      "see\n",
      "here\n",
      "anything\n",
      "less\n",
      "then\n",
      "the\n",
      "fandoms\n",
      "dentist\n",
      "pony\n",
      "i\n",
      "cant\n",
      "be\n",
      "the\n",
      "only\n",
      "dentist\n",
      "who\n",
      "couldnt\n",
      "care\n",
      "less\n",
      "about\n",
      "orthodontics\n",
      "first\n",
      "dentist\n",
      "to\n",
      "become\n",
      "a\n",
      "chief\n",
      "minister\n",
      "they\n",
      "do\n",
      "the\n",
      "same\n",
      "with\n",
      "mexico\n",
      "americans\n",
      "are\n",
      "usually\n",
      "complaining\n",
      "about\n",
      "mexico\n",
      "but\n",
      "they're\n",
      "constantly\n",
      "there\n",
      "at\n",
      "the\n",
      "dentist\n",
      "and\n",
      "getting\n",
      "prescriptions\n",
      "went\n",
      "to\n",
      "dentist\n",
      "today\n",
      "and\n",
      "his\n",
      "face\n",
      "was\n",
      "so\n",
      "funny\n",
      "i\n",
      "started\n",
      "cracking\n",
      "up\n",
      "really\n",
      "had\n",
      "to\n",
      "make\n",
      "up\n",
      "a\n",
      "whole\n",
      "story\n",
      "as\n",
      "to\n",
      "why\n",
      "i\n",
      "am\n",
      "laughing\n",
      "really\n",
      "gotta\n",
      "stop\n",
      "laughing\n",
      "in\n",
      "serious\n",
      "situation\n",
      "feeling\n",
      "very\n",
      "dentist\n",
      "stuffies\n",
      "w\n",
      "teeth\n",
      "horrorcore\n",
      "rn\n",
      "omgg\n",
      "that’s\n",
      "so\n",
      "cool\n",
      "if\n",
      "u\n",
      "became\n",
      "a\n",
      "dentist\n",
      "i’m\n",
      "rooting\n",
      "for\n",
      "u\n",
      "lt;3\n",
      "…and\n",
      "for\n",
      "me\n",
      "i\n",
      "wanna\n",
      "study\n",
      "fashion\n",
      "sadly\n",
      "long\n",
      "line\n",
      "at\n",
      "the\n",
      "dentist\n",
      "dentists\n",
      "will\n",
      "love\n",
      "jake's\n",
      "teeth\n",
      "me\n",
      "thinks\n",
      "yup\n",
      "and\n",
      "also\n",
      "the\n",
      "dentist\n",
      "scene\n",
      "when\n",
      "anne\n",
      "was\n",
      "like\n",
      "\"domino\n",
      "is\n",
      "the\n",
      "alpha\n",
      "and\n",
      "the\n",
      "omega\"\n",
      "was\n",
      "that\n",
      "a\n",
      "foreshadowing\n",
      "of\n",
      "a\n",
      "literal\n",
      "god\n",
      "takinythe\n",
      "form\n",
      "of\n",
      "domino\n",
      "twit\n",
      "content\n",
      "i’m\n",
      "scared\n",
      "to\n",
      "step\n",
      "out\n",
      "the\n",
      "car\n",
      "and\n",
      "look\n",
      "at\n",
      "the\n",
      "car\n",
      "people\n",
      "with\n",
      "loud\n",
      "cars\n",
      "wanna\n",
      "be\n",
      "noticed\n",
      "so\n",
      "bad\n",
      "want\n",
      "mercedes\n",
      "the\n",
      "engineering\n",
      "design\n",
      "lead\n",
      "of\n",
      "the\n",
      "original\n",
      "range\n",
      "rover\n",
      "spen\n",
      "king\n",
      "said\n",
      "sadly\n",
      "the\n",
      "4x4\n",
      "has\n",
      "become\n",
      "an\n",
      "alternative\n",
      "to\n",
      "a\n",
      "mercedes\n",
      "or\n",
      "bmw\n",
      "for\n",
      "the\n",
      "pompous\n",
      "self-important\n",
      "driver\n",
      "mercedes\n",
      "araoz\n",
      "el\n",
      "mercedes\n",
      "benz\n",
      "i\n",
      "love\n",
      "everything\n",
      "about\n",
      "this\n",
      "mercedes\n",
      "love\n",
      "mercedes\n",
      "so\n",
      "much\n",
      "thank\n",
      "you\n",
      "mercedes\n",
      "-\n",
      "canto\n",
      "para\n",
      "ti\n",
      "wishing\n",
      "all\n",
      "buddhists\n",
      "a\n",
      "very\n",
      "happy\n",
      "wesak\n",
      "day\n",
      "yeah\n",
      "man\n",
      "i\n",
      "barely\n",
      "watch\n",
      "football\n",
      "these\n",
      "days\n",
      "morning\n",
      "football\n",
      "and\n",
      "syllabus\n",
      "revision\n",
      "football\n",
      "is\n",
      "over\n",
      "between\n",
      "the\n",
      "two\n",
      "i’m\n",
      "happy\n",
      "for\n",
      "my\n",
      "seniors\n",
      "next\n",
      "year\n",
      "i’m\n",
      "a\n",
      "proud\n",
      "coach\n",
      "don’t\n",
      "wear\n",
      "baseball\n",
      "shirts\n",
      "to\n",
      "a\n",
      "football\n",
      "game\n",
      "a18\n",
      "bro\n",
      "i\n",
      "swear\n",
      "i\n",
      "miss\n",
      "football\n",
      "so\n",
      "much\n",
      "aye\n",
      "hope\n",
      "you\n",
      "enjoy\n",
      "an\n",
      "undermanned\n",
      "was\n",
      "happy\n",
      "to\n",
      "come\n",
      "away\n",
      "with\n",
      "the\n",
      "four\n",
      "points\n",
      "against\n",
      "craigieburn\n",
      "i\n",
      "love\n",
      "football\n",
      "love\n",
      "everything\n",
      "about\n",
      "this\n",
      "art\n",
      "thank\n",
      "you\n",
      "i\n",
      "love\n",
      "your\n",
      "art\n",
      "i\n",
      "love\n",
      "this\n",
      "art\n",
      "style\n",
      "woah\n",
      "this\n",
      "looks\n",
      "like\n",
      "official\n",
      "concept\n",
      "art\n",
      "oh\n",
      "god\n",
      "i\n",
      "hate\n",
      "that\n",
      "ship\n",
      "this\n",
      "looks\n",
      "like\n",
      "a\n",
      "really\n",
      "cool\n",
      "project\n",
      "omg\n",
      "i\n",
      "love\n",
      "your\n",
      "art\n",
      "thank\n",
      "you\n",
      "so\n",
      "much\n",
      "my\n",
      "love\n",
      "thank\n",
      "you\n",
      "so\n",
      "much\n",
      "my\n",
      "love\n",
      "i\n",
      "love\n",
      "your\n",
      "art\n"
     ]
    }
   ],
   "source": [
    "word2 = []\n",
    "for line in text_tweets:\n",
    "    wordlist = line.split()\n",
    "    for word in wordlist:\n",
    "        word = word.lower()\n",
    "        word = word.strip(\"’.,;%$&#!'?\")\n",
    "        word2.append(word)\n",
    "        if word.isdigit():\n",
    "            word2.remove(word) \n",
    "        else:    \n",
    "            print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are providing you with a Python script named `preprocess.py`. The script `preprocess.py` accepts one argument on the command line: a JSON file with tweets (i.e., `tweets.json`). You can run the program like this:\n",
    "\n",
    "`$ python3 preprocess.py tweets.json`\n",
    "\n",
    "**There are some parts specified in this script that you need to implement**. The goal of this script is to clean all the tweets in `tweets.json`. Running `preprocess.py` will generate an output file named `clean_tweets.txt` containing **one string per line** containing a clean tweet. The order of the clean tweets in your output file should follow the order of the lines in the original `tweets.json`. Basically, the first line in `clean_tweets.txt` should correspond to the first raw tweet in `tweets.json`, the second line should correspond to the second tweet, and so on. If you perform any sorting or you put the processed data in a dictionary the order will not be preserved. Once again: **The n-th line of `clean_tweets.txt` (the file you will submit) should be a string that represent the clean version of the n-the line in the `tweets.json` (the input file).**\n",
    "\n",
    "You must provide a line for **every** tweet. If the clean tweet is the empty string then just provide a line with the empty string.\n",
    "\n",
    "***What to turn in: The file `clean_tweets.txt` output by `preprocess.py` after you have implemented the missing parts in `preprocess.py`.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Derive the sentiment of each tweet [40 points]\n",
    "\n",
    "For this part, you will compute the sentiment of each clean tweet in `clean_tweets.txt` based on the sentiment scores of the terms in the tweet. The sentiment of a tweet is equivalent to the sum of the sentiment scores for each term in the clean tweet.\n",
    "\n",
    "You are provided with a skeleton file `tweet_sentiment.py` which accepts two arguments on the command line: a *sentiment file* and a tweet file like the one you generated in Question 1. You can run the skeleton program like this:\n",
    "\n",
    "`$ python3 tweet_sentiment.py AFINN-111.txt clean_tweets.txt`\n",
    "\n",
    "The file `AFINN-111.txt` contains a list of pre-computed sentiment scores. Each line in the file contains a word or phrase phollowed by a sentiment score. Each word or phrase that is found in a tweet but not found in `AFINN-111.txt` should be given a sentiment score of 0. See the file `AFINN-README.txt` for more information.\n",
    "\n",
    "To use the data in the `AFINN-111.txt` file, you may find it useful to build a dictionary. Note that the `AFINN-111.txt` file format is tab-delimited, meaning that the term and the score are separated by a tab character. A tab character corresponds to the string \"\\t\". The following snipped of code may be useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "afinnfile_name = open(\"AFINN-111.txt\")\n",
    "afinnfile = open(\"AFINN-111.txt\", 'r')\n",
    "scores = {} # initialize an empty dictionary\n",
    "for line in afinnfile:\n",
    "    term, score = line.split(\"\\t\") # The file is tab-delimited and \"\\t\" means tab character\n",
    "    scores[term] = int(score) # Conver the score to an integer. It was parsed as a string.\n",
    "afinnfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you 0\n",
      "fantasize 0\n",
      "about 0\n",
      "dating 0\n",
      "a 0\n",
      "dentist 0\n",
      "at 0\n",
      "the 0\n",
      "dentist 0\n",
      "will 0\n",
      "you 0\n",
      "stop -1\n",
      "raining 0\n",
      "let 0\n",
      "me 0\n",
      "go 0\n",
      "to 0\n",
      "my 0\n",
      "dentist 0\n",
      "if 0\n",
      "you 0\n",
      "are 0\n",
      "naughty 0\n",
      "i 0\n",
      "shall 0\n",
      "bite 0\n",
      "you 0\n",
      "and 0\n",
      "the 0\n",
      "dentist 0\n",
      "tells 0\n",
      "me 0\n",
      "i 0\n",
      "have 0\n",
      "one 0\n",
      "of 0\n",
      "the 0\n",
      "strongest 2\n",
      "bites 0\n",
      "he 0\n",
      "knows 0\n",
      "so 0\n",
      "beware 0\n",
      "this 0\n",
      "is 0\n",
      "the 0\n",
      "adult 0\n",
      "take 0\n",
      "dentists 0\n",
      "do 0\n",
      "have 0\n",
      "emergencies 0\n",
      "the 0\n",
      "dentist 0\n",
      "offers 0\n",
      "treatments 0\n",
      "options 0\n",
      "be 0\n",
      "sure 0\n",
      "that 0\n",
      "the 0\n",
      "dentist 0\n",
      "provides 0\n",
      "treatment 0\n",
      "options 0\n",
      "that 0\n",
      "fit 1\n",
      "your 0\n",
      "specific 0\n",
      "needs 0\n",
      "and 0\n",
      "your 0\n",
      "budget 0\n",
      "coast 0\n",
      "dental 0\n",
      "gives 0\n",
      "every 0\n",
      "patient 0\n",
      "a 0\n",
      "written 0\n",
      "treatment 0\n",
      "plan 0\n",
      "to 0\n",
      "review 0\n",
      "before 0\n",
      "any 0\n",
      "procedure 0\n",
      "begins 0\n",
      "visit 0\n",
      "yr 0\n",
      "dentist 0\n",
      "every 0\n",
      "months 0\n",
      "if 0\n",
      "you 0\n",
      "can 0\n",
      "dentist 0\n",
      "lawrence 0\n",
      "neville 0\n",
      "reveals 0\n",
      "foods 0\n",
      "to 0\n",
      "avoid -1\n",
      "if 0\n",
      "you 0\n",
      "want 1\n",
      "a 0\n",
      "perfect 3\n",
      "smile 2\n",
      "will 0\n",
      "never 0\n",
      "see 0\n",
      "here 0\n",
      "anything 0\n",
      "less 0\n",
      "then 0\n",
      "the 0\n",
      "fandoms 0\n",
      "dentist 0\n",
      "pony 0\n",
      "i 0\n",
      "cant 0\n",
      "be 0\n",
      "the 0\n",
      "only 0\n",
      "dentist 0\n",
      "who 0\n",
      "couldnt 0\n",
      "care 2\n",
      "less 0\n",
      "about 0\n",
      "orthodontics 0\n",
      "first 0\n",
      "dentist 0\n",
      "to 0\n",
      "become 0\n",
      "a 0\n",
      "chief 0\n",
      "minister 0\n",
      "they 0\n",
      "do 0\n",
      "the 0\n",
      "same 0\n",
      "with 0\n",
      "mexico 0\n",
      "americans 0\n",
      "are 0\n",
      "usually 0\n",
      "complaining 0\n",
      "about 0\n",
      "mexico 0\n",
      "but 0\n",
      "they're 0\n",
      "constantly 0\n",
      "there 0\n",
      "at 0\n",
      "the 0\n",
      "dentist 0\n",
      "and 0\n",
      "getting 0\n",
      "prescriptions 0\n",
      "went 0\n",
      "to 0\n",
      "dentist 0\n",
      "today 0\n",
      "and 0\n",
      "his 0\n",
      "face 0\n",
      "was 0\n",
      "so 0\n",
      "funny 4\n",
      "i 0\n",
      "started 0\n",
      "cracking 0\n",
      "up 0\n",
      "really 0\n",
      "had 0\n",
      "to 0\n",
      "make 0\n",
      "up 0\n",
      "a 0\n",
      "whole 0\n",
      "story 0\n",
      "as 0\n",
      "to 0\n",
      "why 0\n",
      "i 0\n",
      "am 0\n",
      "laughing 1\n",
      "really 0\n",
      "gotta 0\n",
      "stop -1\n",
      "laughing 1\n",
      "in 0\n",
      "serious 0\n",
      "situation 0\n",
      "feeling 1\n",
      "very 0\n",
      "dentist 0\n",
      "stuffies 0\n",
      "w 0\n",
      "teeth 0\n",
      "horrorcore 0\n",
      "rn 0\n",
      "omgg 0\n",
      "that’s 0\n",
      "so 0\n",
      "cool 1\n",
      "if 0\n",
      "u 0\n",
      "became 0\n",
      "a 0\n",
      "dentist 0\n",
      "i’m 0\n",
      "rooting 0\n",
      "for 0\n",
      "u 0\n",
      "lt;3 0\n",
      "…and 0\n",
      "for 0\n",
      "me 0\n",
      "i 0\n",
      "wanna 0\n",
      "study 0\n",
      "fashion 0\n",
      "sadly -2\n",
      "long 0\n",
      "line 0\n",
      "at 0\n",
      "the 0\n",
      "dentist 0\n",
      "dentists 0\n",
      "will 0\n",
      "love 3\n",
      "jake's 0\n",
      "teeth 0\n",
      "me 0\n",
      "thinks 0\n",
      "yup 0\n",
      "and 0\n",
      "also 0\n",
      "the 0\n",
      "dentist 0\n",
      "scene 0\n",
      "when 0\n",
      "anne 0\n",
      "was 0\n",
      "like 2\n",
      "\"domino 0\n",
      "is 0\n",
      "the 0\n",
      "alpha 0\n",
      "and 0\n",
      "the 0\n",
      "omega\" 0\n",
      "was 0\n",
      "that 0\n",
      "a 0\n",
      "foreshadowing 0\n",
      "of 0\n",
      "a 0\n",
      "literal 0\n",
      "god 1\n",
      "takinythe 0\n",
      "form 0\n",
      "of 0\n",
      "domino 0\n",
      "twit 0\n",
      "content 0\n",
      "i’m 0\n",
      "scared -2\n",
      "to 0\n",
      "step 0\n",
      "out 0\n",
      "the 0\n",
      "car 0\n",
      "and 0\n",
      "look 0\n",
      "at 0\n",
      "the 0\n",
      "car 0\n",
      "people 0\n",
      "with 0\n",
      "loud 0\n",
      "cars 0\n",
      "wanna 0\n",
      "be 0\n",
      "noticed 0\n",
      "so 0\n",
      "bad -3\n",
      "want 1\n",
      "mercedes 0\n",
      "the 0\n",
      "engineering 0\n",
      "design 0\n",
      "lead 0\n",
      "of 0\n",
      "the 0\n",
      "original 0\n",
      "range 0\n",
      "rover 0\n",
      "spen 0\n",
      "king 0\n",
      "said 0\n",
      "sadly -2\n",
      "the 0\n",
      "4x4 0\n",
      "has 0\n",
      "become 0\n",
      "an 0\n",
      "alternative 0\n",
      "to 0\n",
      "a 0\n",
      "mercedes 0\n",
      "or 0\n",
      "bmw 0\n",
      "for 0\n",
      "the 0\n",
      "pompous 0\n",
      "self-important 0\n",
      "driver 0\n",
      "mercedes 0\n",
      "araoz 0\n",
      "el 0\n",
      "mercedes 0\n",
      "benz 0\n",
      "i 0\n",
      "love 3\n",
      "everything 0\n",
      "about 0\n",
      "this 0\n",
      "mercedes 0\n",
      "love 3\n",
      "mercedes 0\n",
      "so 0\n",
      "much 0\n",
      "thank 2\n",
      "you 0\n",
      "mercedes 0\n",
      "- 0\n",
      "canto 0\n",
      "para 0\n",
      "ti 0\n",
      "wishing 1\n",
      "all 0\n",
      "buddhists 0\n",
      "a 0\n",
      "very 0\n",
      "happy 3\n",
      "wesak 0\n",
      "day 0\n",
      "yeah 1\n",
      "man 0\n",
      "i 0\n",
      "barely 0\n",
      "watch 0\n",
      "football 0\n",
      "these 0\n",
      "days 0\n",
      "morning 0\n",
      "football 0\n",
      "and 0\n",
      "syllabus 0\n",
      "revision 0\n",
      "football 0\n",
      "is 0\n",
      "over 0\n",
      "between 0\n",
      "the 0\n",
      "two 0\n",
      "i’m 0\n",
      "happy 3\n",
      "for 0\n",
      "my 0\n",
      "seniors 0\n",
      "next 0\n",
      "year 0\n",
      "i’m 0\n",
      "a 0\n",
      "proud 2\n",
      "coach 0\n",
      "don’t 0\n",
      "wear 0\n",
      "baseball 0\n",
      "shirts 0\n",
      "to 0\n",
      "a 0\n",
      "football 0\n",
      "game 0\n",
      "a18 0\n",
      "bro 0\n",
      "i 0\n",
      "swear -2\n",
      "i 0\n",
      "miss -2\n",
      "football 0\n",
      "so 0\n",
      "much 0\n",
      "aye 0\n",
      "hope 2\n",
      "you 0\n",
      "enjoy 2\n",
      "an 0\n",
      "undermanned 0\n",
      "was 0\n",
      "happy 3\n",
      "to 0\n",
      "come 0\n",
      "away 0\n",
      "with 0\n",
      "the 0\n",
      "four 0\n",
      "points 0\n",
      "against 0\n",
      "craigieburn 0\n",
      "i 0\n",
      "love 3\n",
      "football 0\n",
      "love 3\n",
      "everything 0\n",
      "about 0\n",
      "this 0\n",
      "art 0\n",
      "thank 2\n",
      "you 0\n",
      "i 0\n",
      "love 3\n",
      "your 0\n",
      "art 0\n",
      "i 0\n",
      "love 3\n",
      "this 0\n",
      "art 0\n",
      "style 0\n",
      "woah 0\n",
      "this 0\n",
      "looks 0\n",
      "like 2\n",
      "official 0\n",
      "concept 0\n",
      "art 0\n",
      "oh 0\n",
      "god 1\n",
      "i 0\n",
      "hate -3\n",
      "that 0\n",
      "ship 0\n",
      "this 0\n",
      "looks 0\n",
      "like 2\n",
      "a 0\n",
      "really 0\n",
      "cool 1\n",
      "project 0\n",
      "omg 0\n",
      "i 0\n",
      "love 3\n",
      "your 0\n",
      "art 0\n",
      "thank 2\n",
      "you 0\n",
      "so 0\n",
      "much 0\n",
      "my 0\n",
      "love 3\n",
      "thank 2\n",
      "you 0\n",
      "so 0\n",
      "much 0\n",
      "my 0\n",
      "love 3\n",
      "i 0\n",
      "love 3\n",
      "your 0\n",
      "art 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(word2)):\n",
    "    if word2[i] not in list(scores.keys()):\n",
    "        print(word2[i],0)\n",
    "    else:\n",
    "        h = str(scores[(word2[i])])\n",
    "        print(word2[i],scores[(word2[i])])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  you fantasize about dating a dentist     0\n",
      "at the dentist      0\n",
      "will you stop raining let me go to my dentist     -1\n",
      "if you are naughty, i shall bite you. and the dentist tells me i have one of the strongest bites he knows. so beware     2\n",
      "  this is the adult take     0\n",
      "dentists do have emergencies     0\n",
      "the dentist offers treatments options be sure that the dentist provides treatment options that fit your specific needs and your budget. coast dental gives every patient a written treatment plan to review before any procedure begins.       1\n",
      "  visit yr dentist every 6 months if you can      0\n",
      "dentist lawrence neville reveals foods to avoid if you want a perfect smile       5\n",
      "  will never see here anything less then the fandoms dentist pony       0\n",
      "i cant be the only dentist who couldnt care less about orthodontics     2\n",
      "first dentist to become a chief minister.       0\n",
      "they do the same with mexico. americans are usually complaining about mexico but they're constantly there at the dentist and getting prescriptions     0\n",
      "went to dentist today and his face was so funny i started cracking up, really had to make up a whole story as to why i am laughing really gotta stop laughing in serious situation      5\n",
      "feeling very dentist stuffies w teeth horrorcore rn       1\n",
      "omgg that’s so cool if u became a dentist!!! i’m rooting for u &lt;3 …and for me i wanna study fashion     1\n",
      "sadly... long line at the dentist     0\n",
      "dentists will love jake's teeth me thinks     3\n",
      "  yup and also the dentist scene when anne was like \"domino is the alpha and the omega\" ...was that a foreshadowing of a literal god takinythe form of domino     3\n",
      "twit content     0\n",
      "i’m scared to step out the car and look at the car      -2\n",
      "people with loud cars wanna be noticed so bad      -3\n",
      "want #mercedes       1\n",
      "the engineering design lead of the original range rover, spen king said, 'sadly, the 4x4 has become an alternative to a mercedes or bmw for the pompous, self-important driver.      0\n",
      "  mercedes araoz      0\n",
      "el mercedes benz       0\n",
      "i love everything about this mercedes  love mercedes so much        6\n",
      "  thank you     2\n",
      "mercedes - canto para ti       0\n",
      "wishing all buddhists a very happy wesak day     4\n",
      "yeah man, i barely watch football these days     1\n",
      "  morning football and syllabus revision     0\n",
      "  football is over between the two     0\n",
      "i’m happy for my seniors next year, i’m a proud coach      5\n",
      "don’t wear baseball shirts to a football game. a18     0\n",
      "bro i swear i miss football so much     -4\n",
      "  aye hope you enjoy     4\n",
      "an undermanned   was happy to come away with the four points against craigieburn     3\n",
      "i love football     3\n",
      "  love everything about this art      3\n",
      "  thank you! i love your art     5\n",
      "i love this art style     3\n",
      "  woah!! this looks like official concept art     2\n",
      "oh god i hate that ship     -2\n",
      "  this looks like a really cool project     3\n",
      "  omg i love your art     3\n",
      "  thank you so much, my love     5\n",
      "  thank you so much, my love     5\n",
      "  i love your art     3\n"
     ]
    }
   ],
   "source": [
    "#SUM of tweets\n",
    "sentimentfile = open(\"sum_tweets.txt\",\"w\")\n",
    "sum_tweets = {}\n",
    "list_notin_doc = []\n",
    "for key in text_tweets :\n",
    "    summ = 0\n",
    "    word = key.split()\n",
    "    for text in word :\n",
    "        if text in scores.keys() :\n",
    "            summ = scores[text] + summ\n",
    "        else:\n",
    "            summ = 0 + summ\n",
    "            list_notin_doc.append(text)\n",
    "    sum_tweets[key]= summ               \n",
    "    print(key,\"   \",summ) \n",
    "    #print(list_notin_doc)\n",
    "    sentimentfile.write( \"%s\\n\" % summ)                       \n",
    "sentimentfile.close()                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your script should output a file named `sentiment.txt` containing the sentiment of each tweet in the file `clean_tweets.txt`, one numeric sentiment score per line. The first score should correspond to the first tweet, the second score should correspond to the second tweet, and so on. In other words, ** the n-th line of the file you submit should contain only a single number that represents teh score of the n-th tweet in the input file.**\n",
    "\n",
    "After you have implemented everything the first 10 lines of the generated output of your script should be exactly the same as the next lines:\n",
    "\n",
    "```\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "0\n",
    "1\n",
    "2\n",
    "-4\n",
    "0\n",
    "0\n",
    "```\n",
    "\n",
    "***What to turn in: The file `sentiment.txt` after you have verified that it returns the correct answers***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Derive the sentiment of new terms [40 points]\n",
    "\n",
    "In this part you will create a script that computes the sentiment for terms that **do not** appear in the file `AFINN-111.txt`.\n",
    "\n",
    "You can think about this problem as follows: We know we can use the sentiment-carrying words in `AFINN-111.txt` to deduce the overall sentiment of a tweet. Once you deduce the sentiment of a tweet, you can work backwards to deduce the sentiment of the non-sentiment carrying words that *do not appear* in `AFINN-111.txt`. For example, if the word *football* always appears in proximity with positive words like *great* and *fun*, then we can deduce that the term *football* itself carried a positive sentiment.\n",
    "\n",
    "You are provided with a skeleton file `term_sentiment.py` which accepts the same two arguments as `tweet_sentiment.py` and can be executed using the following command:\n",
    "\n",
    "`$ python3 term_sentiment.py AFINN-111.txt clean_tweets.txt`\n",
    "\n",
    "Your script should print its output to stdout. Each line of the output should contain a term, followed by a space, followed by a sentiment. That is, each line should be in the format <term:string> <sentiment:float>. For example if you have the pair (\"foo\", 54.2) in Python, it should appear in the output as: `foo 54.2`.\n",
    "\n",
    "*The order of your output does not matter.*\n",
    "\n",
    "***What to turn in: The file `term_sentiment.py` after you have implemented the missing parts.***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "lisst = [\"you\",\"about\",\"a\",\"at\",\"the\",\"will\",\"me\",\"to\",\"if\",\"are\",\"i\",\"and\",\"have\",\"has\",\"he\",\n",
    "         \"so\",\"this\",\"is\",\"do\",\"be\",\"that\",\"your\",\"any\",\"yr\",\"who\",\"where\",\"with\",\"but\",\"at\",\n",
    "         \"his\",\"here\",\"up\",\"as\",\"why\",\"am\",\"in\",\"w\",\"rn\",\"i’m\",\"also\",\"when\",\"was\",\"were\",\"is\",\"your\",\"omg\",\"yup\",\"you!\",\"woah!!\",\n",
    "         \"8\",\"bro\",\"a1\",\"4x4\",\"for\",\"out\",\"...was\",\"sadly...\",\"…and\",\"&lt;3\",\"you.\",\"then\",\"they\",\"they're\",\"before\",\n",
    "        \"don’t\",\"wanna\",\"much,\",\"much\",\"self-important\",\"whole\",\"never\",\"these\",\"next\",\"days\",\"a18\",\"had\",\"up\",\"all\",\"that’s\",\"jake's\",\n",
    "        \"year\",\"omgg\",\"y\",\"first\",\"up,\",\"day\",\"between\",\"over\",\"very\",\"one\",\"two\",\"dentists\",\"dentist\",\"your\",\"so\",\"#mercedes\",\"'sadly,\",\"dentist!!!\",'\"domino']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fantasize', 'dating', 'raining', 'let', 'naughty,', 'shall', 'bite', 'tells', 'bites', 'knows.', 'beware', 'adult', 'take', 'emergencies', 'offers', 'treatments', 'options', 'sure', 'provides', 'treatment', 'options', 'specific', 'needs', 'budget.', 'coast', 'dental', 'gives', 'every', 'patient', 'written', 'treatment', 'plan', 'review', 'procedure', 'begins.', 'visit', 'every', 'months', 'can', 'lawrence', 'neville', 'reveals', 'foods', 'see', 'anything', 'less', 'fandoms', 'pony', 'cant', 'only', 'couldnt', 'less', 'orthodontics', 'become', 'chief', 'minister.', 'same', 'mexico.', 'americans', 'usually', 'complaining', 'mexico', 'constantly', 'there', 'getting', 'prescriptions', 'went', 'today', 'face', 'started', 'cracking', 'really', 'make', 'story', 'really', 'gotta', 'serious', 'situation', 'stuffies', 'teeth', 'horrorcore', 'became', 'rooting', 'study', 'fashion', 'long', 'line', 'teeth', 'thinks', 'scene', 'anne', 'alpha', 'omega\"', 'foreshadowing', 'literal', 'takinythe', 'form', 'domino', 'twit', 'content', 'step', 'car', 'look', 'car', 'people', 'loud', 'cars', 'noticed', 'engineering', 'design', 'lead', 'original', 'range', 'rover,', 'spen', 'king', 'said,', 'become', 'alternative', 'mercedes', 'bmw', 'pompous,', 'driver.', 'mercedes', 'araoz', 'mercedes', 'benz', 'everything', 'mercedes', 'mercedes', 'mercedes', 'canto', 'para', 'buddhists', 'wesak', 'man,', 'barely', 'watch', 'football', 'morning', 'football', 'syllabus', 'revision', 'football', 'seniors', 'year,', 'coach', 'wear', 'baseball', 'shirts', 'football', 'game.', 'football', 'aye', 'undermanned', 'come', 'away', 'four', 'points', 'against', 'craigieburn', 'football', 'everything', 'art', 'art', 'art', 'style', 'looks', 'official', 'concept', 'art', 'ship', 'looks', 'really', 'project', 'art', 'art']\n"
     ]
    }
   ],
   "source": [
    "for n in list_notin_doc :\n",
    "    if len(n) <= 2  or n in lisst :\n",
    "        list_notin_doc.remove(n)  \n",
    "print(list_notin_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fantasize', 0], ['dating', 0], ['raining', -1], ['let', -1], ['naughty,', 2], ['shall', 2], ['bite', 2], ['tells', 2], ['bites', 2], ['knows.', 2], ['beware', 2], ['adult', 0], ['take', 0], ['emergencies', 0], ['offers', 1], ['treatments', 1], ['options', 1], ['sure', 1], ['provides', 1], ['treatment', 1], ['options', 1], ['specific', 1], ['needs', 1], ['budget.', 1], ['coast', 1], ['dental', 1], ['gives', 1], ['every', 1], ['patient', 1], ['written', 1], ['treatment', 1], ['plan', 1], ['review', 1], ['procedure', 1], ['begins.', 1], ['visit', 0], ['every', 0], ['months', 0], ['can', 0], ['lawrence', 5], ['neville', 5], ['reveals', 5], ['foods', 5], ['see', 0], ['anything', 0], ['less', 0], ['fandoms', 0], ['pony', 0], ['cant', 2], ['only', 2], ['couldnt', 2], ['less', 2], ['orthodontics', 2], ['become', 0], ['chief', 0], ['minister.', 0], ['same', 0], ['mexico.', 0], ['americans', 0], ['usually', 0], ['complaining', 0], ['mexico', 0], ['constantly', 0], ['there', 0], ['getting', 0], ['prescriptions', 0], ['went', 5], ['today', 5], ['face', 5], ['started', 5], ['cracking', 5], ['really', 5], ['make', 5], ['story', 5], ['really', 5], ['gotta', 5], ['serious', 5], ['situation', 5], ['stuffies', 1], ['teeth', 1], ['horrorcore', 1], ['became', 1], ['rooting', 1], ['study', 1], ['fashion', 1], ['long', 0], ['line', 0], ['teeth', 3], ['thinks', 3], ['scene', 3], ['anne', 3], ['alpha', 3], ['omega\"', 3], ['foreshadowing', 3], ['literal', 3], ['takinythe', 3], ['form', 3], ['domino', 3], ['twit', 0], ['content', 0], ['step', -2], ['car', -2], ['look', -2], ['car', -2], ['people', -3], ['loud', -3], ['cars', -3], ['noticed', -3], ['engineering', 0], ['design', 0], ['lead', 0], ['original', 0], ['range', 0], ['rover,', 0], ['spen', 0], ['king', 0], ['said,', 0], ['become', 0], ['alternative', 0], ['mercedes', 0], ['bmw', 0], ['pompous,', 0], ['driver.', 0], ['mercedes', 0], ['araoz', 0], ['mercedes', 0], ['benz', 0], ['everything', 6], ['mercedes', 6], ['mercedes', 6], ['mercedes', 0], ['canto', 0], ['para', 0], ['buddhists', 4], ['wesak', 4], ['man,', 1], ['barely', 1], ['watch', 1], ['football', 1], ['morning', 0], ['football', 0], ['syllabus', 0], ['revision', 0], ['football', 0], ['seniors', 5], ['year,', 5], ['coach', 5], ['wear', 0], ['baseball', 0], ['shirts', 0], ['football', 0], ['game.', 0], ['football', -4], ['aye', 4], ['undermanned', 3], ['come', 3], ['away', 3], ['four', 3], ['points', 3], ['against', 3], ['craigieburn', 3], ['football', 3], ['everything', 3], ['art', 3], ['art', 5], ['art', 3], ['style', 3], ['looks', 2], ['official', 2], ['concept', 2], ['art', 2], ['ship', -2], ['looks', 3], ['really', 3], ['project', 3], ['art', 3], ['art', 3]]\n"
     ]
    }
   ],
   "source": [
    "listt1 = []\n",
    "for key in  sum_tweets.keys() :\n",
    "    n = key.split()\n",
    "    for y in n :\n",
    "        if y in list_notin_doc :\n",
    "            listt1.append([y,sum_tweets[key]])\n",
    "print(listt1)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value of tweets</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fantasize</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dating</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raining</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>let</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>naughty,</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shall</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bite</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tells</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bites</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knows.</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beware</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adult</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>take</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emergencies</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offers</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treatments</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>options</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sure</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>provides</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treatment</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             value of tweets\n",
       "word                        \n",
       "fantasize                  0\n",
       "dating                     0\n",
       "raining                   -1\n",
       "let                       -1\n",
       "naughty,                   2\n",
       "shall                      2\n",
       "bite                       2\n",
       "tells                      2\n",
       "bites                      2\n",
       "knows.                     2\n",
       "beware                     2\n",
       "adult                      0\n",
       "take                       0\n",
       "emergencies                0\n",
       "offers                     1\n",
       "treatments                 1\n",
       "options                    1\n",
       "sure                       1\n",
       "provides                   1\n",
       "treatment                  1"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame((listt1),columns=(\"word\",\"value of tweets\"))\n",
    "df.set_index(\"word\").head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value of tweets</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adult</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>against</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alternative</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>americans</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             value of tweets  prediction\n",
       "word                                    \n",
       "adult                      0           1\n",
       "against                    3           1\n",
       "alpha                      3           1\n",
       "alternative                0           1\n",
       "americans                  0           1"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_notindic = open(\"word.txt\",\"w\")\n",
    "df[\"prediction\"] = np.where(df[\"value of tweets\"] >= 0,1,-1)\n",
    "df2 = df.groupby(\"word\").sum()\n",
    "#word_notindic.write(str(df2[\"prediction\"]))\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
